{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4db076db-afbf-4894-bea8-3297107a0cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-08 13:50:58 [__init__.py:256] Automatically detected platform cuda.\n",
      "WARNING 07-08 13:50:59 [config.py:2603] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 07-08 13:51:10 [config.py:583] This model supports multiple tasks: {'score', 'embed', 'classify', 'generate', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 07-08 13:51:10 [config.py:1519] Defaulting to use mp for distributed inference\n",
      "INFO 07-08 13:51:10 [config.py:1697] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 07-08 13:51:10 [cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "[2025-07-08 13:51:11,648] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-08 13:51:11,744 - root - INFO - gcc -pthread -B /root/miniconda3/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /root/miniconda3/include -fPIC -O2 -isystem /root/miniconda3/include -fPIC -c /tmp/tmpix0xfmhc/test.c -o /tmp/tmpix0xfmhc/test.o\n",
      "2025-07-08 13:51:11,759 - root - INFO - gcc -pthread -B /root/miniconda3/compiler_compat /tmp/tmpix0xfmhc/test.o -laio -o /tmp/tmpix0xfmhc/a.out\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 07-08 13:51:12 [utils.py:2148] CUDA was previously initialized. We must use the `spawn` multiprocessing start method. Setting VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing for more information.\n",
      "INFO 07-08 13:51:18 [__init__.py:256] Automatically detected platform cuda.\n",
      "[2025-07-08 13:51:20,584] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "INFO 07-08 13:51:21 [core.py:53] Initializing a V1 LLM engine (v0.8.1) with config: model='/root/models/DeepResearch_Models/DeepResearcher-7b/', speculative_config=None, tokenizer='/root/models/DeepResearch_Models/DeepResearcher-7b/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=15120, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/root/models/DeepResearch_Models/DeepResearcher-7b/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}\n",
      "INFO 07-08 13:51:21 [shm_broadcast.py:258] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 10485760, 10, 'psm_b774bfaf'), local_subscribe_addr='ipc:///tmp/5837e572-f270-4c81-b136-efee1837fa4c', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 07-08 13:51:28 [__init__.py:256] Automatically detected platform cuda.\n",
      "[2025-07-08 13:51:29,874] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "WARNING 07-08 13:51:31 [utils.py:2282] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f69646f8ee0>\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=253185)\u001b[0;0m INFO 07-08 13:51:31 [shm_broadcast.py:258] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_a2d48c8a'), local_subscribe_addr='ipc:///tmp/befc0001-e195-45f0-9a11-f511a3d77cbd', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 07-08 13:51:37 [__init__.py:256] Automatically detected platform cuda.\n",
      "[2025-07-08 13:51:39,747] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "WARNING 07-08 13:51:40 [utils.py:2282] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f83c8875960>\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=253247)\u001b[0;0m INFO 07-08 13:51:40 [shm_broadcast.py:258] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_15cb5851'), local_subscribe_addr='ipc:///tmp/bd4bc02f-a599-49ef-bda9-95b33634f252', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 07-08 13:51:47 [__init__.py:256] Automatically detected platform cuda.\n",
      "[2025-07-08 13:51:49,023] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "WARNING 07-08 13:51:50 [utils.py:2282] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f9e10d81180>\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=253318)\u001b[0;0m INFO 07-08 13:51:50 [shm_broadcast.py:258] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_50e0d776'), local_subscribe_addr='ipc:///tmp/85cdab2a-1be3-4cfa-a47c-82752270a344', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 07-08 13:51:56 [__init__.py:256] Automatically detected platform cuda.\n",
      "[2025-07-08 13:51:58,580] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "WARNING 07-08 13:51:59 [utils.py:2282] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f83ac91d180>\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=253589)\u001b[0;0m INFO 07-08 13:51:59 [shm_broadcast.py:258] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_fb86f000'), local_subscribe_addr='ipc:///tmp/2def9f84-df19-4408-833b-4d2d18ecdd98', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=253318)\u001b[0;0m INFO 07-08 13:52:00 [utils.py:925] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=253589)\u001b[0;0m INFO 07-08 13:52:00 [utils.py:925] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=253247)\u001b[0;0m INFO 07-08 13:52:00 [utils.py:925] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=253318)\u001b[0;0m INFO 07-08 13:52:00 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=253589)\u001b[0;0m INFO 07-08 13:52:00 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=253247)\u001b[0;0m INFO 07-08 13:52:00 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=253185)\u001b[0;0m INFO 07-08 13:52:00 [utils.py:925] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=253185)\u001b[0;0m INFO 07-08 13:52:00 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=253185)\u001b[0;0m INFO 07-08 13:52:02 [custom_all_reduce_utils.py:206] generating GPU P2P access cache in /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=253589)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=253185)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=253318)\u001b[0;0m INFO 07-08 13:52:31 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "INFO 07-08 13:52:31 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "INFO 07-08 13:52:31 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=253247)\u001b[0;0m INFO 07-08 13:52:31 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=253185)\u001b[0;0m INFO 07-08 13:52:31 [shm_broadcast.py:258] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_3e000f9c'), local_subscribe_addr='ipc:///tmp/b4ba433f-8542-4f03-bfe3-7b14bd6f4d40', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=253589)\u001b[0;0m INFO 07-08 13:52:31 [parallel_state.py:967] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=253318)\u001b[0;0m INFO 07-08 13:52:31 [parallel_state.py:967] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=253185)\u001b[0;0m INFO 07-08 13:52:31 [parallel_state.py:967] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=253247)\u001b[0;0m INFO 07-08 13:52:31 [parallel_state.py:967] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=253589)\u001b[0;0m INFO 07-08 13:52:31 [cuda.py:215] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=253185)\u001b[0;0m INFO 07-08 13:52:31 [cuda.py:215] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=253247)\u001b[0;0m INFO 07-08 13:52:31 [cuda.py:215] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=253318)\u001b[0;0m INFO 07-08 13:52:31 [cuda.py:215] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=253318)\u001b[0;0m INFO 07-08 13:52:31 [gpu_model_runner.py:1164] Starting to load model /root/models/DeepResearch_Models/DeepResearcher-7b/...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=253185)\u001b[0;0m INFO 07-08 13:52:31 [gpu_model_runner.py:1164] Starting to load model /root/models/DeepResearch_Models/DeepResearcher-7b/...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=253247)\u001b[0;0m INFO 07-08 13:52:31 [gpu_model_runner.py:1164] Starting to load model /root/models/DeepResearch_Models/DeepResearcher-7b/...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=253589)\u001b[0;0m INFO 07-08 13:52:31 [gpu_model_runner.py:1164] Starting to load model /root/models/DeepResearch_Models/DeepResearcher-7b/...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=253318)\u001b[0;0m INFO 07-08 13:52:31 [topk_topp_sampler.py:38] Currently, FlashInfer top-p & top-k sampling sampler is disabled because FlashInfer>=v0.2.3 is not backward compatible. Falling back to the PyTorch-native implementation of top-p & top-k sampling.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=253589)\u001b[0;0m INFO 07-08 13:52:31 [topk_topp_sampler.py:38] Currently, FlashInfer top-p & top-k sampling sampler is disabled because FlashInfer>=v0.2.3 is not backward compatible. Falling back to the PyTorch-native implementation of top-p & top-k sampling.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=253185)\u001b[0;0m INFO 07-08 13:52:31 [topk_topp_sampler.py:38] Currently, FlashInfer top-p & top-k sampling sampler is disabled because FlashInfer>=v0.2.3 is not backward compatible. Falling back to the PyTorch-native implementation of top-p & top-k sampling.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=253247)\u001b[0;0m INFO 07-08 13:52:31 [topk_topp_sampler.py:38] Currently, FlashInfer top-p & top-k sampling sampler is disabled because FlashInfer>=v0.2.3 is not backward compatible. Falling back to the PyTorch-native implementation of top-p & top-k sampling.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.24s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.60it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=253589)\u001b[0;0m INFO 07-08 13:52:35 [loader.py:429] Loading weights took 3.65 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=253318)\u001b[0;0m INFO 07-08 13:52:35 [loader.py:429] Loading weights took 3.78 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.05it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.09it/s]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=253185)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=253247)\u001b[0;0m INFO 07-08 13:52:35 [loader.py:429] Loading weights took 3.69 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=253185)\u001b[0;0m INFO 07-08 13:52:35 [loader.py:429] Loading weights took 3.73 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=253318)\u001b[0;0m INFO 07-08 13:52:35 [gpu_model_runner.py:1176] Model loading took 3.5551 GB and 3.936653 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=253589)\u001b[0;0m INFO 07-08 13:52:35 [gpu_model_runner.py:1176] Model loading took 3.5551 GB and 3.898726 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=253247)\u001b[0;0m INFO 07-08 13:52:35 [gpu_model_runner.py:1176] Model loading took 3.5551 GB and 4.124312 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=253185)\u001b[0;0m INFO 07-08 13:52:35 [gpu_model_runner.py:1176] Model loading took 3.5551 GB and 4.165492 seconds\n",
      "INFO 07-08 13:52:40 [kv_cache_utils.py:537] GPU KV cache size: 4,635,680 tokens\n",
      "INFO 07-08 13:52:40 [kv_cache_utils.py:540] Maximum concurrency for 15,120 tokens per request: 306.59x\n",
      "INFO 07-08 13:52:40 [kv_cache_utils.py:537] GPU KV cache size: 4,635,680 tokens\n",
      "INFO 07-08 13:52:40 [kv_cache_utils.py:540] Maximum concurrency for 15,120 tokens per request: 306.59x\n",
      "INFO 07-08 13:52:40 [kv_cache_utils.py:537] GPU KV cache size: 4,635,680 tokens\n",
      "INFO 07-08 13:52:40 [kv_cache_utils.py:540] Maximum concurrency for 15,120 tokens per request: 306.59x\n",
      "INFO 07-08 13:52:40 [kv_cache_utils.py:537] GPU KV cache size: 4,635,680 tokens\n",
      "INFO 07-08 13:52:40 [kv_cache_utils.py:540] Maximum concurrency for 15,120 tokens per request: 306.59x\n",
      "INFO 07-08 13:52:41 [core.py:138] init engine (profile, create kv cache, warmup model) took 5.48 seconds\n",
      "oss://yzz123456/temp/data_debug  machineintelligence-55-03314005520233.140.55.202_20250708_1352 注册成功。\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3'\n",
    "import vllm\n",
    "llm = vllm.LLM(\n",
    "    \"/root/models/DeepResearch_Models/DeepResearcher-7b/\",\n",
    "    gpu_memory_utilization=0.95,\n",
    "    tensor_parallel_size=4,\n",
    "    trust_remote_code=True,\n",
    "    dtype=\"half\",\n",
    "    enforce_eager=True,\n",
    "    max_model_len=15120,\n",
    "    disable_log_stats=True,\n",
    ")\n",
    "from tools_server.util import MessageClient\n",
    "import yaml\n",
    "config = yaml.safe_load(open(\"./tools_server/config.yaml\"))\n",
    "client = MessageClient(config['data_writing_path'],\n",
    "                       isconsumer = True,\n",
    "                       oss_access_key_id=config['oss_access_key_id'],\n",
    "                       oss_access_key_secret=config['oss_access_key_secret'],\n",
    "                       oss_endpoint=config['oss_endpoint'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ebf4e2c-7459-4cfa-813f-a5a9e3932322",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.12s/it, est. speed input: 585.32 toks/s, output: 64.54 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 content is  \n",
      "为了回答这个问题，我需要首先找到周杰伦最喜欢哪位NBA球员。之后需要了解这位球员的历史生涯中获得了多少枚总冠军戒指。我将首先进行搜索来获取这些信息。\n",
      "</think>\n",
      "<tool_call>\n",
      "{\"name\": \"web_search\", \"arguments\": {\"query\": \"周杰伦最喜欢的NBA球员\"}}\n",
      "</tool_call>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.17s/it, est. speed input: 2491.39 toks/s, output: 59.01 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 content is  \n",
      "从搜索结果中可以看到周杰伦最喜欢的NBA球员是迈克尔乔丹。接下来我需要了解迈克尔乔丹在他的职业生涯中获得了多少枚总冠军戒指。\n",
      "</think>\n",
      "<tool_call>\n",
      "{\"name\": \"web_search\", \"arguments\": {\"query\": \"迈克尔乔丹获得过多少个NBA总冠军\"}}\n",
      "</tool_call>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.16s/it, est. speed input: 4177.21 toks/s, output: 31.90 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 content is  \n",
      "搜索结果中多次提到迈克尔乔丹获得了6次NBA总冠军。结合这些信息，我可以得出答案。\n",
      "</think>\n",
      "<answer>\n",
      "6\n",
      "</answer>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "system_prompt = r'''## Background information \n",
    "* Today is 2025-04-12\n",
    "* You are Deep AI Research Assistant\n",
    "\n",
    "The question I give you is a complex question that requires a *deep research* to answer.\n",
    "\n",
    "I will provide you with two tools to help you answer the question:\n",
    "* A web search tool to help you perform google search. \n",
    "* A webpage browsing tool to help you get new page content.\n",
    "\n",
    "You don't have to answer the question now, but you should first think about the research plan or what to search next.\n",
    "\n",
    "Your output format should be one of the following two formats:\n",
    "\n",
    "<think>\n",
    "YOUR THINKING PROCESS\n",
    "</think>\n",
    "<answer>\n",
    "YOUR ANSWER AFTER GETTING ENOUGH INFORMATION\n",
    "</answer>\n",
    "\n",
    "or\n",
    "\n",
    "<think>\n",
    "YOUR THINKING PROCESS\n",
    "</think>\n",
    "<tool_call>\n",
    "YOUR TOOL CALL WITH CORRECT FORMAT\n",
    "</tool_call>\n",
    " hubn8n 8  8 i8ib ii kb8hn\n",
    "You should always follow the above two formats strictly.\n",
    "Only output the final answer (in words, numbers or phrase) inside the <answer></answer> tag, without any explanations or extra information. If this is a yes-or-no question, you should only answer yes or no.\n",
    "\n",
    "\n",
    "# Tools\n",
    "\n",
    "You may call one or more functions to assist with the user query.\n",
    "\n",
    "You are provided with function signatures within <tools></tools> XML tags:\n",
    "<tools>\n",
    "{\"type\": \"function\", \"function\": {\"name\": \"web_search\", \"description\": \"Search the web for relevant information from google. You should use this tool if the historical page content is not enough to answer the question. Or last search result is not relevant to the question.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"query\": {\"type\": \"string\", \"description\": \"The query to search, which helps answer the question\"}}, \"required\": [\"query\"], \"minItems\": 1, \"uniqueItems\": true}}}\n",
    "{\"type\": \"function\", \"function\": {\"name\": \"browse_webpage\", \"description\": \"Browse the webpage and return the content that not appeared in the conversation history. You should use this tool if the last action is search and the search result maybe relevant to the question.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"url_list\": {\"type\": \"array\", \"description\": \"The chosen urls from the search result, do not use url that not appeared in the search result. Do not use more than 3 urls.\"}, \"query\": {\"type\": \"string\", \"description\": \"These queries aim to retrieve information from the URL webpage.\"}}, \"example\": {\"name\": \"browse_webpage\", \"arguments\": {\"url_list\": [\"http://www.dada.com\", \"xxxx\"], \"query\": \"xxxx\"}}, \"uniqueItems\": true}}}\n",
    "</tools>\n",
    "\n",
    "For each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\n",
    "<tool_call>\n",
    "{\"name\": <function-name>, \"arguments\": <args-json-object>}\n",
    "</tool_call>'''\n",
    "\n",
    "tokenizer = llm.get_tokenizer()\n",
    "def parse_response(response_contents, think: bool = True):\n",
    "    \"\"\"Parse response to get the thinking process and answer or tool call.\n",
    "        return: [(is_stop, thinking, answer/tool_call), ...]\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for i, content in enumerate(response_contents):\n",
    "        print(f'{i} content is {content}')\n",
    "        if think:\n",
    "            content = \"<think>\" + content\n",
    "        if \"<think>\" in content and \"<answer>\" in content:\n",
    "            if \"</think>\" not in content or \"</answer>\" not in content:\n",
    "                results.append((True, \"\", \"\"))\n",
    "            else:\n",
    "                think = content.split(\"<think>\")[1].split(\"</think>\")[0]\n",
    "                answer = content.split(\"<answer>\")[1].split(\"</answer>\")[0]\n",
    "                results.append((True, think, answer))\n",
    "        elif \"<think>\" in content and \"<tool_call>\" in content:\n",
    "            if \"</tool_call>\" not in content or \"</think>\" not in content:\n",
    "                results.append((True, \"\", \"\"))\n",
    "            else:\n",
    "                think = content.split(\"<think>\")[1].split(\"</think>\")[0]\n",
    "                tool_call = content.split(\"<tool_call>\")[1].split(\"</tool_call>\")[0]\n",
    "                try:\n",
    "                    tool_call = json.loads(tool_call)\n",
    "                    results.append((False, think, tool_call))\n",
    "                except Exception as e:\n",
    "                    print(f\"model tool call format error: {e}\")\n",
    "                    print(i, content)\n",
    "                    results.append((True, \"\", \"\"))\n",
    "        else:\n",
    "            results.append((True, \"\", \"\"))\n",
    "    return results\n",
    "\n",
    "\n",
    "def execute_predictions(\n",
    "        tool_call_list, total_number\n",
    "    ) :\n",
    "    query_contents = [{\"idx\": tool_call[0], \"question\": tool_call[1], \"think\": tool_call[2],\n",
    "                       \"tool_call\": tool_call[3], \"total_number\":total_number} for tool_call in tool_call_list]\n",
    "\n",
    "    query_contents = client.submit_tasks(query_contents)\n",
    "    return query_contents\n",
    "    \n",
    "def run_question(question):\n",
    "    i = 0\n",
    "    messages_list = []\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    messages_list.append(messages)\n",
    "    activate_list = [0]\n",
    "    while(i < 10):\n",
    "        rollings_active = tokenizer.apply_chat_template(messages_list, add_generation_prompt=True, tokenize=False)\n",
    "        rollings_active = [x + \"<think>\" for x in rollings_active]\n",
    "        # print('----input',rollings_active)\n",
    "        # print(rollings_active[0])\n",
    "        response = llm.generate(\n",
    "            rollings_active,\n",
    "            vllm.SamplingParams(\n",
    "                temperature=1.0,\n",
    "                max_tokens=32000,\n",
    "            ),\n",
    "            use_tqdm=True,\n",
    "        )\n",
    "\n",
    "        \n",
    "        response = [x.outputs[0].text for x in response]\n",
    "        results = parse_response(response)\n",
    "\n",
    "        tool_call_list = []\n",
    "        activate_list_copy = []   \n",
    "\n",
    "        for i in range(len(results)):\n",
    "            if results[i][0]:\n",
    "                messages_list[activate_list[i]].append({\"role\": \"assistant\", \"content\": \"<think>\" + response[i]})\n",
    "            else:\n",
    "                activate_list_copy.append(activate_list[i])\n",
    "                tool_call_list.append((activate_list[i], messages_list[activate_list[i]][1][\"content\"], results[i][1], results[i][2]))\n",
    "                \n",
    "        tool_call_list = execute_predictions(tool_call_list,len(messages_list))\n",
    "        for i in range(len(tool_call_list)):\n",
    "            messages_list[tool_call_list[i]['idx']].append(\n",
    "                {\n",
    "                    \"role\": \"assistant\", \n",
    "                    \"content\": \"<think>\" + tool_call_list[i]['think'] + \"</think>\", \n",
    "                    \"tool_calls\": [\n",
    "                                    {\n",
    "                                        \"type\": \"function\", \n",
    "                                        \"function\": tool_call_list[i]['tool_call']\n",
    "                                    }\n",
    "                                ]\n",
    "                }\n",
    "            )\n",
    "            messages_list[tool_call_list[i]['idx']].append(\n",
    "                {\n",
    "                    \"role\": \"tool\", \n",
    "                    \"name\": tool_call_list[i]['tool_call'][\"name\"],\n",
    "                    \"content\": tool_call_list[i]['content']\n",
    "                }\n",
    "            )\n",
    "        i += 1\n",
    "        activate_list = activate_list_copy\n",
    "        if len(activate_list_copy) == 0:\n",
    "            break\n",
    "    return messages_list\n",
    "\n",
    "    \n",
    "# run_question(\"What is the capital of Columbia County?\") \n",
    "res = run_question(\"周杰伦最喜欢的NBA球员历史生涯一共获得过多少枚总冠军戒指\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98899682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd \n",
    "\n",
    "# test = pd.read_parquet(\"init_sft_train.parquet\")\n",
    "# test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e73c2e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 打开输出文件进行写入\n",
    "# output_file = 'gen_sft.jsonl'\n",
    "# with open(output_file, \"a\", encoding=\"utf-8\") as f_out:\n",
    "#     # 遍历 test['problem'] 列表中的所有问题，跳过已经处理的部分\n",
    "#     count = 0\n",
    "#     for i in range(len(test)):\n",
    "#         q = test['prompts'].values[i]  # 假定 id 列存在\n",
    "#         response = run_question(q) \n",
    "#         new_format_res = {'input':q, 'output': response}  # 包含 id 字段\n",
    "#         f_out.write(json.dumps(new_format_res, ensure_ascii=False) + \"\\n\")\n",
    "#         f_out.flush()\n",
    "#         count += 1\n",
    "#         print(f\"Processed count: {count},  Result: {new_format_res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403ac596",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}


-----

# Atom-Searcher: Enhancing Agentic Deep Research via Fine-Grained Atomic Thought Rewards

[ÁÆÄ‰Ωì‰∏≠Êñá](README_CN.md)

## üìñ Introduction

**Atom-Searcher** is an innovative Reinforcement Learning (RL) framework designed to enhance the performance of Large Language Models (LLMs) in **Agentic Deep Research** tasks.

Although existing agentic research systems can reason and search autonomously, their training relies heavily on sparse, outcome-only reward signals. This approach leads to gradient conflicts and inefficient training, limiting the model's ability to learn optimal research strategies.

To address these challenges, we introduce two core concepts:

1.  **Atomic Thoughts**: We decompose the model's reasoning process into the smallest, functionally meaningful units (e.g., `planning`, `reflection`, `verification`).
2.  **Atomic Thought Reward (ATR)**: We use a Reasoning Reward Model (RRM) to score these atomic thoughts, providing fine-grained, process-level feedback.

Atom-Searcher employs a dynamic reward aggregation strategy inspired by curriculum learning. It initially focuses on the process-level **ATR** and gradually shifts its focus to outcome-based rewards as training progresses. This design effectively mitigates gradient conflicts and reward sparsity, guiding the model to converge on effective reasoning paths more efficiently.

-----

## üöÄ Core Features

  * **Atomic Thought Abstraction**: For the first time, we propose decomposing the LLM's reasoning process into functional "atomic thoughts" and incentivizing the model to induce these thoughts autonomously, enhancing the interpretability of the model's behavior.
  * **Fine-Grained Reward Mechanism**: We designed an Atomic Thought Reward (ATR) to provide dense, meaningful intermediate signals for RL training, addressing the gradient conflict and reward sparsity issues found in traditional methods.
  * **Dynamic Reward Scheduling**: We use dynamically changing weights to aggregate the process reward (ATR) and the outcome reward (F1 score). This allows the reward mechanism to adapt to the model's learning dynamics, focusing on exploration in the early stages and convergence in the later stages.
  * **Superior Performance**: On 7 authoritative question-answering benchmarks (both in-domain and out-of-domain), Atom-Searcher significantly outperforms existing SOTA agentic research models of the same scale.

-----

## üõ†Ô∏è How It Works

The Atom-Searcher framework consists of two stages: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL).

*Figure 1: Overview of the Atom-Searcher framework. First, the model is taught to generate atomic thoughts via SFT. Then, it is optimized using a hybrid reward signal during RL.*

### 1\. Atomic Thoughts

We break down the model's thought process `<think>...</think>` into a series of more fine-grained atomic thoughts, such as `<plan>`, `<reflection>`, `<verification>`, etc. We do not predefine a fixed set of atomic thoughts; instead, we guide the model through SFT to learn how to decompose its reasoning process based on different tasks.

### 2\. Reward Modeling

The final reward $R$ used for RL training is a hybrid signal that dynamically combines the **Atomic Thought Reward** ($R\_{atom}$) and the **Outcome Reward** ($R\_{f1}$).

  - **Atomic Thought Reward ($R\_{atom}$)**: We use a powerful Reasoning Reward Model (RRM, e.g., Qwen3-30B) to evaluate the quality of each atomic thought generated by the model and aggregate these scores into $R\_{atom}$.
  - **Outcome Reward ($R\_{f1}$)**: This is calculated based on the F1 score between the model's final answer and the reference answer.
  - **Dynamic Aggregation**: We use a linearly decaying coefficient $\\alpha$ to balance these two types of rewards.

$$R = \alpha \cdot R_{\text{atom}} + (1 - \alpha) \cdot R_{f1}$$

Where $\\alpha = 0.5 \\times (1 - \\frac{T}{T\_{\\text{MAX}}})$, $T$ is the current training step, and $T\_{\\text{MAX}}$ is the total number of training steps. In the early stages of training, $\\alpha$ is large, making the process reward dominant; as training progresses, $\\alpha$ decreases, and the weight of the outcome reward increases.

### 3\. Reinforcement Learning Training

We use the **GRPO (Group Relative Policy Optimization)** algorithm, combined with the hybrid reward $R$ described above, to optimize the policy model initialized by SFT. Additionally, we introduce the **Sliding Window Entropy Regularization Mechanism (SWERM)** to prevent policy entropy collapse and ensure training stability.

-----

## üìä Main Results

We evaluated Atom-Searcher on 4 in-domain (ID) and 3 out-of-domain (OOD) datasets. The results show that, based on the Qwen2.5-7B-Instruct backbone, Atom-Searcher comprehensively surpasses all baseline models, including DeepResearcher.

| Base Model              | Method                                      | Inference Environment | NQ   | TQ   | HotpotQA | 2Wiki | Musique | Bamboogle | PopQA | agg  |
| :---------------------- | :------------------------------------------ | :-------------------- | :--- | :--- | :------- | :---- | :------ | :-------- | :---- | :--- |
| **Qwen2.5-7B-Instruct** | DeepResearcher                              | Web Search            | 39.6 | 78.4 | 52.8     | 59.7  | 27.1    | 71.0      | 48.5  | 53.9 |
| **Qwen2.5-7B-Instruct** | DeepResearcher\_add\_process\_level\_reward | web\_search           | 40.1 | 78.2 | 53.5     | 60.0  | 25.7    | 70.5      | 48.8  | 53.8 |
| **Qwen2.5-7B-Instruct** | atom\_searcher                              | web search            | 43.8 | 81.8 | 55.7     | 64.6  | 27.6    | 70.7      | 50.3  | 56.4 |

-----

## Model Weights

We have open-sourced the model weights trained with the Atom-Searcher framework. You can download them from the Hugging Face Hub via the following link:

  - **Atom-Searcher (Qwen2.5-7B-Instruct)**: [ü§ó Hugging Face Model (insert your link here)](https://www.google.com/search?q=https://huggingface.co/your-username/your-model-name)

-----

## üöÄ How to Use